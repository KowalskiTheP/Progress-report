[input]


[network]
inputDim = 1
outputDim = 1
neuronsPerLayer = 10,100,1000,2000,3000
activationPerLayer = relu
recurrentActivation = hard_sigmoid
initWeights = uniform
dropout = 0.2
optimiser = adam
learningRate = 0.001
loss = mse
epochs = 10
batchSize = 16 

[tuning]


[output]
